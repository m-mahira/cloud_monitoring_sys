# -*- coding: utf-8 -*-
"""training_model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UQHPmuB_BNiYfx4oFFe2Yr86ZAnLY6p2
"""

#!/usr/bin/env python3
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import IsolationForest
import joblib
import os

# Paths
CSV_PATH    = '/home/malkamahira/data_metrics.csv'
MODEL_PATH  = '/home/malkamahira/isolation_forest_model.joblib'
SCALER_PATH = '/home/malkamahira/scaler.joblib'
OUTPUT_CSV  = '/home/malkamahira/metrics_with_anomalies.csv'

# 1) Load the live metrics CSV
df = pd.read_csv(CSV_PATH)

df = df[pd.to_numeric(df['timestamp'], errors='coerce').notnull()]

# 2) Select the features we configured in Telegraf
feature_cols = [
    'available',
    'total',
    'used'
]
for col in feature_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# Drop any row where ANY of the features is NaN
df = df.dropna(subset=feature_cols)

for col in feature_cols:
    if col not in df.columns:
        raise RuntimeError(f"Missing feature column: {col}")

X = df[feature_cols].values

# 3) Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 4) Train the Isolation Forest
model = IsolationForest(
    contamination=0.05,  # flag ~5% of points as anomalies
    n_estimators=100,
    max_samples='auto',
    random_state=42
)
model.fit(X_scaled)

# 5) Compute anomaly scores and labels
scores = model.decision_function(X_scaled)  # continuous: lower = more anomalous
labels = model.predict(X_scaled)            # -1 = anomaly, +1 = normal

# 6) Save the scaler and model for reuse
joblib.dump(scaler, SCALER_PATH)
joblib.dump(model, MODEL_PATH)

# 7) Append results to the DataFrame and save
df['anomaly_score'] = scores
df['is_anomaly']    = (labels == -1).astype(int)
df.to_csv(OUTPUT_CSV, index=False)

print(f"Trained on {len(df)} samples.")
print(f"Saved model to: {MODEL_PATH}")
print(f"Anomaly results written to: {OUTPUT_CSV}")